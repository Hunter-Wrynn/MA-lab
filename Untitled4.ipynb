{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7955470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, epsilon: 1.00\n",
      "Episode 2/100, epsilon: 0.80\n",
      "Episode 3/100, epsilon: 0.61\n",
      "Episode 4/100, epsilon: 0.47\n",
      "Episode 5/100, epsilon: 0.34\n",
      "Episode 6/100, epsilon: 0.30\n",
      "Episode 7/100, epsilon: 0.28\n",
      "Episode 8/100, epsilon: 0.21\n",
      "Episode 9/100, epsilon: 0.16\n",
      "Episode 10/100, epsilon: 0.12\n",
      "Episode 11/100, epsilon: 0.08\n",
      "Episode 12/100, epsilon: 0.07\n",
      "Episode 13/100, epsilon: 0.05\n",
      "Episode 14/100, epsilon: 0.04\n",
      "Episode 15/100, epsilon: 0.04\n",
      "Episode 16/100, epsilon: 0.03\n",
      "Episode 17/100, epsilon: 0.02\n",
      "Episode 18/100, epsilon: 0.02\n",
      "Episode 19/100, epsilon: 0.01\n",
      "Episode 20/100, epsilon: 0.01\n",
      "Episode 21/100, epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class Gomoku:\n",
    "    def __init__(self, size=15):\n",
    "        self.size = size\n",
    "        self.board = np.zeros((size, size), dtype=int)\n",
    "        self.current_player = 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.board\n",
    "    \n",
    "    def is_valid_move(self, x, y):\n",
    "        return 0 <= x < self.size and 0 <= y < self.size and self.board[x, y] == 0\n",
    "    \n",
    "    def make_move(self, x, y):\n",
    "        if self.is_valid_move(x, y):\n",
    "            self.board[x, y] = self.current_player\n",
    "            self.current_player = 3 - self.current_player\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def check_winner(self):\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if self.board[x, y] != 0 and self.check_direction(x, y):\n",
    "                    return self.board[x, y]\n",
    "        return 0\n",
    "    \n",
    "    def check_direction(self, x, y):\n",
    "        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n",
    "        for d in directions:\n",
    "            count = 0\n",
    "            for i in range(-4, 5):\n",
    "                nx, ny = x + i * d[0], y + i * d[1]\n",
    "                if 0 <= nx < self.size and 0 <= ny < self.size and self.board[nx, ny] == self.board[x, y]:\n",
    "                    count += 1\n",
    "                    if count == 5:\n",
    "                        return True\n",
    "                else:\n",
    "                    count = 0\n",
    "        return False\n",
    "\n",
    "    def evaluate_position(self, player):\n",
    "        \"\"\"Evaluate the board from the perspective of the given player.\"\"\"\n",
    "        score = 0\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if self.board[x, y] == player:\n",
    "                    score += self.evaluate_point(x, y, player)\n",
    "                elif self.board[x, y] == 3 - player:\n",
    "                    score -= self.evaluate_point(x, y, 3 - player)\n",
    "        return score\n",
    "\n",
    "    def evaluate_point(self, x, y, player):\n",
    "        \"\"\"Evaluate a single point from the perspective of the given player.\"\"\"\n",
    "        score = 0\n",
    "        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n",
    "        for d in directions:\n",
    "            count = 0\n",
    "            block = 0\n",
    "            for i in range(-4, 5):\n",
    "                nx, ny = x + i * d[0], y + i * d[1]\n",
    "                if 0 <= nx < self.size and 0 <= ny < self.size:\n",
    "                    if self.board[nx, ny] == player:\n",
    "                        count += 1\n",
    "                    elif self.board[nx, ny] != 0:\n",
    "                        block += 1\n",
    "                        break\n",
    "                else:\n",
    "                    block += 1\n",
    "            if count == 5:\n",
    "                score += 10000  # win\n",
    "            elif count == 4 and block == 0:\n",
    "                score += 100  # open four\n",
    "            elif count == 3 and block == 0:\n",
    "                score += 10  # open three\n",
    "            elif count == 2 and block == 0:\n",
    "                score += 1  # open two\n",
    "        return score\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size=128, gamma=0.99, lr=0.001, batch_size=64, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.model = DQN(state_size, hidden_size, action_size)\n",
    "        self.target_model = DQN(state_size, hidden_size, action_size)  # Add target network\n",
    "        self.update_target_model()  # Initialize target model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.get_valid_actions(state))\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        valid_actions = self.get_valid_actions(state)\n",
    "        return valid_actions[torch.argmax(q_values[0][valid_actions]).item()]\n",
    "    \n",
    "    def get_valid_actions(self, state):\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            state = state.numpy().flatten()  # Convert tensor to numpy array and flatten it\n",
    "        return [i for i in range(self.action_size) if state[i] == 0]\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * torch.max(self.target_model(next_state)).item()\n",
    "            target_f = self.model(state)\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(target_f, self.model(state))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "def train_dqn(agent, env, episodes=100):\n",
    "    for e in range(episodes):\n",
    "        state = env.reset().flatten()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            x, y = divmod(action, env.size)\n",
    "            if env.is_valid_move(x, y):\n",
    "                env.make_move(x, y)\n",
    "                reward = env.evaluate_position(1)\n",
    "                next_state = env.board.flatten()\n",
    "                done = env.check_winner() > 0 or (env.board != 0).all()\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "                # Opponent's turn (simple heuristic)\n",
    "                opp_action = random.choice(agent.get_valid_actions(env.board.flatten()))\n",
    "                opp_x, opp_y = divmod(opp_action, env.size)\n",
    "                env.make_move(opp_x, opp_y)\n",
    "                if env.check_winner():\n",
    "                    reward = -10000  # Large negative reward for losing\n",
    "                    done = True\n",
    "            else:\n",
    "                reward = -100  # Penalty for invalid move\n",
    "                done = True\n",
    "            agent.replay()\n",
    "        agent.update_target_model()  # Update target network periodically\n",
    "        print(f\"Episode {e+1}/{episodes}, epsilon: {agent.epsilon:.2f}\")\n",
    "    agent.save_model('dqn_gomoku.pth')\n",
    "\n",
    "gomoku_env = Gomoku()\n",
    "dqn_agent = DQNAgent(state_size=gomoku_env.size*gomoku_env.size, action_size=gomoku_env.size*gomoku_env.size)\n",
    "train_dqn(dqn_agent, gomoku_env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a55764b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, epsilon: 1.00\n",
      "Episode 2/500, epsilon: 1.00\n",
      "Episode 3/500, epsilon: 1.00\n",
      "Episode 4/500, epsilon: 1.00\n",
      "Episode 5/500, epsilon: 1.00\n",
      "Episode 6/500, epsilon: 1.00\n",
      "Episode 7/500, epsilon: 1.00\n",
      "Episode 8/500, epsilon: 1.00\n",
      "Episode 9/500, epsilon: 0.98\n",
      "Episode 10/500, epsilon: 0.94\n",
      "Episode 11/500, epsilon: 0.93\n",
      "Episode 12/500, epsilon: 0.88\n",
      "Episode 13/500, epsilon: 0.85\n",
      "Episode 14/500, epsilon: 0.80\n",
      "Episode 15/500, epsilon: 0.76\n",
      "Episode 16/500, epsilon: 0.72\n",
      "Episode 17/500, epsilon: 0.69\n",
      "Episode 18/500, epsilon: 0.64\n",
      "Episode 19/500, epsilon: 0.61\n",
      "Episode 20/500, epsilon: 0.58\n",
      "Episode 21/500, epsilon: 0.55\n",
      "Episode 22/500, epsilon: 0.52\n",
      "Episode 23/500, epsilon: 0.51\n",
      "Episode 24/500, epsilon: 0.49\n",
      "Episode 25/500, epsilon: 0.47\n",
      "Episode 26/500, epsilon: 0.46\n",
      "Episode 27/500, epsilon: 0.45\n",
      "Episode 28/500, epsilon: 0.44\n",
      "Episode 29/500, epsilon: 0.41\n",
      "Episode 30/500, epsilon: 0.40\n",
      "Episode 31/500, epsilon: 0.39\n",
      "Episode 32/500, epsilon: 0.38\n",
      "Episode 33/500, epsilon: 0.36\n",
      "Episode 34/500, epsilon: 0.34\n",
      "Episode 35/500, epsilon: 0.32\n",
      "Episode 36/500, epsilon: 0.31\n",
      "Episode 37/500, epsilon: 0.30\n",
      "Episode 38/500, epsilon: 0.29\n",
      "Episode 39/500, epsilon: 0.27\n",
      "Episode 40/500, epsilon: 0.26\n",
      "Episode 41/500, epsilon: 0.26\n",
      "Episode 42/500, epsilon: 0.24\n",
      "Episode 43/500, epsilon: 0.23\n",
      "Episode 44/500, epsilon: 0.23\n",
      "Episode 45/500, epsilon: 0.22\n",
      "Episode 46/500, epsilon: 0.21\n",
      "Episode 47/500, epsilon: 0.21\n",
      "Episode 48/500, epsilon: 0.20\n",
      "Episode 49/500, epsilon: 0.19\n",
      "Episode 50/500, epsilon: 0.19\n",
      "Episode 51/500, epsilon: 0.18\n",
      "Episode 52/500, epsilon: 0.17\n",
      "Episode 53/500, epsilon: 0.16\n",
      "Episode 54/500, epsilon: 0.15\n",
      "Episode 55/500, epsilon: 0.14\n",
      "Episode 56/500, epsilon: 0.14\n",
      "Episode 57/500, epsilon: 0.13\n",
      "Episode 58/500, epsilon: 0.12\n",
      "Episode 59/500, epsilon: 0.12\n",
      "Episode 60/500, epsilon: 0.11\n",
      "Episode 61/500, epsilon: 0.11\n",
      "Episode 62/500, epsilon: 0.10\n",
      "Episode 63/500, epsilon: 0.10\n",
      "Episode 64/500, epsilon: 0.09\n",
      "Episode 65/500, epsilon: 0.09\n",
      "Episode 66/500, epsilon: 0.08\n",
      "Episode 67/500, epsilon: 0.08\n",
      "Episode 68/500, epsilon: 0.08\n",
      "Episode 69/500, epsilon: 0.07\n",
      "Episode 70/500, epsilon: 0.07\n",
      "Episode 71/500, epsilon: 0.07\n",
      "Episode 72/500, epsilon: 0.06\n",
      "Episode 73/500, epsilon: 0.06\n",
      "Episode 74/500, epsilon: 0.06\n",
      "Episode 75/500, epsilon: 0.06\n",
      "Episode 76/500, epsilon: 0.05\n",
      "Episode 77/500, epsilon: 0.05\n",
      "Episode 78/500, epsilon: 0.05\n",
      "Episode 79/500, epsilon: 0.05\n",
      "Episode 80/500, epsilon: 0.05\n",
      "Episode 81/500, epsilon: 0.04\n",
      "Episode 82/500, epsilon: 0.04\n",
      "Episode 83/500, epsilon: 0.04\n",
      "Episode 84/500, epsilon: 0.04\n",
      "Episode 85/500, epsilon: 0.04\n",
      "Episode 86/500, epsilon: 0.04\n",
      "Episode 87/500, epsilon: 0.03\n",
      "Episode 88/500, epsilon: 0.03\n",
      "Episode 89/500, epsilon: 0.03\n",
      "Episode 90/500, epsilon: 0.03\n",
      "Episode 91/500, epsilon: 0.03\n",
      "Episode 92/500, epsilon: 0.03\n",
      "Episode 93/500, epsilon: 0.03\n",
      "Episode 94/500, epsilon: 0.02\n",
      "Episode 95/500, epsilon: 0.02\n",
      "Episode 96/500, epsilon: 0.02\n",
      "Episode 97/500, epsilon: 0.02\n",
      "Episode 98/500, epsilon: 0.02\n",
      "Episode 99/500, epsilon: 0.02\n",
      "Episode 100/500, epsilon: 0.02\n",
      "Episode 101/500, epsilon: 0.02\n",
      "Episode 102/500, epsilon: 0.02\n",
      "Episode 103/500, epsilon: 0.02\n",
      "Episode 104/500, epsilon: 0.02\n",
      "Episode 105/500, epsilon: 0.01\n",
      "Episode 106/500, epsilon: 0.01\n",
      "Episode 107/500, epsilon: 0.01\n",
      "Episode 108/500, epsilon: 0.01\n",
      "Episode 109/500, epsilon: 0.01\n",
      "Episode 110/500, epsilon: 0.01\n",
      "Episode 111/500, epsilon: 0.01\n",
      "Episode 112/500, epsilon: 0.01\n",
      "Episode 113/500, epsilon: 0.01\n",
      "Episode 114/500, epsilon: 0.01\n",
      "Episode 115/500, epsilon: 0.01\n",
      "Episode 116/500, epsilon: 0.01\n",
      "Episode 117/500, epsilon: 0.01\n",
      "Episode 118/500, epsilon: 0.01\n",
      "Episode 119/500, epsilon: 0.01\n",
      "Episode 120/500, epsilon: 0.01\n",
      "Episode 121/500, epsilon: 0.01\n",
      "Episode 122/500, epsilon: 0.01\n",
      "Episode 123/500, epsilon: 0.01\n",
      "Episode 124/500, epsilon: 0.01\n",
      "Episode 125/500, epsilon: 0.01\n",
      "Episode 126/500, epsilon: 0.01\n",
      "Episode 127/500, epsilon: 0.01\n",
      "Episode 128/500, epsilon: 0.01\n",
      "Episode 129/500, epsilon: 0.01\n",
      "Episode 130/500, epsilon: 0.01\n",
      "Episode 131/500, epsilon: 0.01\n",
      "Episode 132/500, epsilon: 0.01\n",
      "Episode 133/500, epsilon: 0.01\n",
      "Episode 134/500, epsilon: 0.01\n",
      "Episode 135/500, epsilon: 0.01\n",
      "Episode 136/500, epsilon: 0.01\n",
      "Episode 137/500, epsilon: 0.01\n",
      "Episode 138/500, epsilon: 0.01\n",
      "Episode 139/500, epsilon: 0.01\n",
      "Episode 140/500, epsilon: 0.01\n",
      "Episode 141/500, epsilon: 0.01\n",
      "Episode 142/500, epsilon: 0.01\n",
      "Episode 143/500, epsilon: 0.01\n",
      "Episode 144/500, epsilon: 0.01\n",
      "Episode 145/500, epsilon: 0.01\n",
      "Episode 146/500, epsilon: 0.01\n",
      "Episode 147/500, epsilon: 0.01\n",
      "Episode 148/500, epsilon: 0.01\n",
      "Episode 149/500, epsilon: 0.01\n",
      "Episode 150/500, epsilon: 0.01\n",
      "Episode 151/500, epsilon: 0.01\n",
      "Episode 152/500, epsilon: 0.01\n",
      "Episode 153/500, epsilon: 0.01\n",
      "Episode 154/500, epsilon: 0.01\n",
      "Episode 155/500, epsilon: 0.01\n",
      "Episode 156/500, epsilon: 0.01\n",
      "Episode 157/500, epsilon: 0.01\n",
      "Episode 158/500, epsilon: 0.01\n",
      "Episode 159/500, epsilon: 0.01\n",
      "Episode 160/500, epsilon: 0.01\n",
      "Episode 161/500, epsilon: 0.01\n",
      "Episode 162/500, epsilon: 0.01\n",
      "Episode 163/500, epsilon: 0.01\n",
      "Episode 164/500, epsilon: 0.01\n",
      "Episode 165/500, epsilon: 0.01\n",
      "Episode 166/500, epsilon: 0.01\n",
      "Episode 167/500, epsilon: 0.01\n",
      "Episode 168/500, epsilon: 0.01\n",
      "Episode 169/500, epsilon: 0.01\n",
      "Episode 170/500, epsilon: 0.01\n",
      "Episode 171/500, epsilon: 0.01\n",
      "Episode 172/500, epsilon: 0.01\n",
      "Episode 173/500, epsilon: 0.01\n",
      "Episode 174/500, epsilon: 0.01\n",
      "Episode 175/500, epsilon: 0.01\n",
      "Episode 176/500, epsilon: 0.01\n",
      "Episode 177/500, epsilon: 0.01\n",
      "Episode 178/500, epsilon: 0.01\n",
      "Episode 179/500, epsilon: 0.01\n",
      "Episode 180/500, epsilon: 0.01\n",
      "Episode 181/500, epsilon: 0.01\n",
      "Episode 182/500, epsilon: 0.01\n",
      "Episode 183/500, epsilon: 0.01\n",
      "Episode 184/500, epsilon: 0.01\n",
      "Episode 185/500, epsilon: 0.01\n",
      "Episode 186/500, epsilon: 0.01\n",
      "Episode 187/500, epsilon: 0.01\n",
      "Episode 188/500, epsilon: 0.01\n",
      "Episode 189/500, epsilon: 0.01\n",
      "Episode 190/500, epsilon: 0.01\n",
      "Episode 191/500, epsilon: 0.01\n",
      "Episode 192/500, epsilon: 0.01\n",
      "Episode 193/500, epsilon: 0.01\n",
      "Episode 194/500, epsilon: 0.01\n",
      "Episode 195/500, epsilon: 0.01\n",
      "Episode 196/500, epsilon: 0.01\n",
      "Episode 197/500, epsilon: 0.01\n",
      "Episode 198/500, epsilon: 0.01\n",
      "Episode 199/500, epsilon: 0.01\n",
      "Episode 200/500, epsilon: 0.01\n",
      "Episode 201/500, epsilon: 0.01\n",
      "Episode 202/500, epsilon: 0.01\n",
      "Episode 203/500, epsilon: 0.01\n",
      "Episode 204/500, epsilon: 0.01\n",
      "Episode 205/500, epsilon: 0.01\n",
      "Episode 206/500, epsilon: 0.01\n",
      "Episode 207/500, epsilon: 0.01\n",
      "Episode 208/500, epsilon: 0.01\n",
      "Episode 209/500, epsilon: 0.01\n",
      "Episode 210/500, epsilon: 0.01\n",
      "Episode 211/500, epsilon: 0.01\n",
      "Episode 212/500, epsilon: 0.01\n",
      "Episode 213/500, epsilon: 0.01\n",
      "Episode 214/500, epsilon: 0.01\n",
      "Episode 215/500, epsilon: 0.01\n",
      "Episode 216/500, epsilon: 0.01\n",
      "Episode 217/500, epsilon: 0.01\n",
      "Episode 218/500, epsilon: 0.01\n",
      "Episode 219/500, epsilon: 0.01\n",
      "Episode 220/500, epsilon: 0.01\n",
      "Episode 221/500, epsilon: 0.01\n",
      "Episode 222/500, epsilon: 0.01\n",
      "Episode 223/500, epsilon: 0.01\n",
      "Episode 224/500, epsilon: 0.01\n",
      "Episode 225/500, epsilon: 0.01\n",
      "Episode 226/500, epsilon: 0.01\n",
      "Episode 227/500, epsilon: 0.01\n",
      "Episode 228/500, epsilon: 0.01\n",
      "Episode 229/500, epsilon: 0.01\n",
      "Episode 230/500, epsilon: 0.01\n",
      "Episode 231/500, epsilon: 0.01\n",
      "Episode 232/500, epsilon: 0.01\n",
      "Episode 233/500, epsilon: 0.01\n",
      "Episode 234/500, epsilon: 0.01\n",
      "Episode 235/500, epsilon: 0.01\n",
      "Episode 236/500, epsilon: 0.01\n",
      "Episode 237/500, epsilon: 0.01\n",
      "Episode 238/500, epsilon: 0.01\n",
      "Episode 239/500, epsilon: 0.01\n",
      "Episode 240/500, epsilon: 0.01\n",
      "Episode 241/500, epsilon: 0.01\n",
      "Episode 242/500, epsilon: 0.01\n",
      "Episode 243/500, epsilon: 0.01\n",
      "Episode 244/500, epsilon: 0.01\n",
      "Episode 245/500, epsilon: 0.01\n",
      "Episode 246/500, epsilon: 0.01\n",
      "Episode 247/500, epsilon: 0.01\n",
      "Episode 248/500, epsilon: 0.01\n",
      "Episode 249/500, epsilon: 0.01\n",
      "Episode 250/500, epsilon: 0.01\n",
      "Episode 251/500, epsilon: 0.01\n",
      "Episode 252/500, epsilon: 0.01\n",
      "Episode 253/500, epsilon: 0.01\n",
      "Episode 254/500, epsilon: 0.01\n",
      "Episode 255/500, epsilon: 0.01\n",
      "Episode 256/500, epsilon: 0.01\n",
      "Episode 257/500, epsilon: 0.01\n",
      "Episode 258/500, epsilon: 0.01\n",
      "Episode 259/500, epsilon: 0.01\n",
      "Episode 260/500, epsilon: 0.01\n",
      "Episode 261/500, epsilon: 0.01\n",
      "Episode 262/500, epsilon: 0.01\n",
      "Episode 263/500, epsilon: 0.01\n",
      "Episode 264/500, epsilon: 0.01\n",
      "Episode 265/500, epsilon: 0.01\n",
      "Episode 266/500, epsilon: 0.01\n",
      "Episode 267/500, epsilon: 0.01\n",
      "Episode 268/500, epsilon: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 269/500, epsilon: 0.01\n",
      "Episode 270/500, epsilon: 0.01\n",
      "Episode 271/500, epsilon: 0.01\n",
      "Episode 272/500, epsilon: 0.01\n",
      "Episode 273/500, epsilon: 0.01\n",
      "Episode 274/500, epsilon: 0.01\n",
      "Episode 275/500, epsilon: 0.01\n",
      "Episode 276/500, epsilon: 0.01\n",
      "Episode 277/500, epsilon: 0.01\n",
      "Episode 278/500, epsilon: 0.01\n",
      "Episode 279/500, epsilon: 0.01\n",
      "Episode 280/500, epsilon: 0.01\n",
      "Episode 281/500, epsilon: 0.01\n",
      "Episode 282/500, epsilon: 0.01\n",
      "Episode 283/500, epsilon: 0.01\n",
      "Episode 284/500, epsilon: 0.01\n",
      "Episode 285/500, epsilon: 0.01\n",
      "Episode 286/500, epsilon: 0.01\n",
      "Episode 287/500, epsilon: 0.01\n",
      "Episode 288/500, epsilon: 0.01\n",
      "Episode 289/500, epsilon: 0.01\n",
      "Episode 290/500, epsilon: 0.01\n",
      "Episode 291/500, epsilon: 0.01\n",
      "Episode 292/500, epsilon: 0.01\n",
      "Episode 293/500, epsilon: 0.01\n",
      "Episode 294/500, epsilon: 0.01\n",
      "Episode 295/500, epsilon: 0.01\n",
      "Episode 296/500, epsilon: 0.01\n",
      "Episode 297/500, epsilon: 0.01\n",
      "Episode 298/500, epsilon: 0.01\n",
      "Episode 299/500, epsilon: 0.01\n",
      "Episode 300/500, epsilon: 0.01\n",
      "Episode 301/500, epsilon: 0.01\n",
      "Episode 302/500, epsilon: 0.01\n",
      "Episode 303/500, epsilon: 0.01\n",
      "Episode 304/500, epsilon: 0.01\n",
      "Episode 305/500, epsilon: 0.01\n",
      "Episode 306/500, epsilon: 0.01\n",
      "Episode 307/500, epsilon: 0.01\n",
      "Episode 308/500, epsilon: 0.01\n",
      "Episode 309/500, epsilon: 0.01\n",
      "Episode 310/500, epsilon: 0.01\n",
      "Episode 311/500, epsilon: 0.01\n",
      "Episode 312/500, epsilon: 0.01\n",
      "Episode 313/500, epsilon: 0.01\n",
      "Episode 314/500, epsilon: 0.01\n",
      "Episode 315/500, epsilon: 0.01\n",
      "Episode 316/500, epsilon: 0.01\n",
      "Episode 317/500, epsilon: 0.01\n",
      "Episode 318/500, epsilon: 0.01\n",
      "Episode 319/500, epsilon: 0.01\n",
      "Episode 320/500, epsilon: 0.01\n",
      "Episode 321/500, epsilon: 0.01\n",
      "Episode 322/500, epsilon: 0.01\n",
      "Episode 323/500, epsilon: 0.01\n",
      "Episode 324/500, epsilon: 0.01\n",
      "Episode 325/500, epsilon: 0.01\n",
      "Episode 326/500, epsilon: 0.01\n",
      "Episode 327/500, epsilon: 0.01\n",
      "Episode 328/500, epsilon: 0.01\n",
      "Episode 329/500, epsilon: 0.01\n",
      "Episode 330/500, epsilon: 0.01\n",
      "Episode 331/500, epsilon: 0.01\n",
      "Episode 332/500, epsilon: 0.01\n",
      "Episode 333/500, epsilon: 0.01\n",
      "Episode 334/500, epsilon: 0.01\n",
      "Episode 335/500, epsilon: 0.01\n",
      "Episode 336/500, epsilon: 0.01\n",
      "Episode 337/500, epsilon: 0.01\n",
      "Episode 338/500, epsilon: 0.01\n",
      "Episode 339/500, epsilon: 0.01\n",
      "Episode 340/500, epsilon: 0.01\n",
      "Episode 341/500, epsilon: 0.01\n",
      "Episode 342/500, epsilon: 0.01\n",
      "Episode 343/500, epsilon: 0.01\n",
      "Episode 344/500, epsilon: 0.01\n",
      "Episode 345/500, epsilon: 0.01\n",
      "Episode 346/500, epsilon: 0.01\n",
      "Episode 347/500, epsilon: 0.01\n",
      "Episode 348/500, epsilon: 0.01\n",
      "Episode 349/500, epsilon: 0.01\n",
      "Episode 350/500, epsilon: 0.01\n",
      "Episode 351/500, epsilon: 0.01\n",
      "Episode 352/500, epsilon: 0.01\n",
      "Episode 353/500, epsilon: 0.01\n",
      "Episode 354/500, epsilon: 0.01\n",
      "Episode 355/500, epsilon: 0.01\n",
      "Episode 356/500, epsilon: 0.01\n",
      "Episode 357/500, epsilon: 0.01\n",
      "Episode 358/500, epsilon: 0.01\n",
      "Episode 359/500, epsilon: 0.01\n",
      "Episode 360/500, epsilon: 0.01\n",
      "Episode 361/500, epsilon: 0.01\n",
      "Episode 362/500, epsilon: 0.01\n",
      "Episode 363/500, epsilon: 0.01\n",
      "Episode 364/500, epsilon: 0.01\n",
      "Episode 365/500, epsilon: 0.01\n",
      "Episode 366/500, epsilon: 0.01\n",
      "Episode 367/500, epsilon: 0.01\n",
      "Episode 368/500, epsilon: 0.01\n",
      "Episode 369/500, epsilon: 0.01\n",
      "Episode 370/500, epsilon: 0.01\n",
      "Episode 371/500, epsilon: 0.01\n",
      "Episode 372/500, epsilon: 0.01\n",
      "Episode 373/500, epsilon: 0.01\n",
      "Episode 374/500, epsilon: 0.01\n",
      "Episode 375/500, epsilon: 0.01\n",
      "Episode 376/500, epsilon: 0.01\n",
      "Episode 377/500, epsilon: 0.01\n",
      "Episode 378/500, epsilon: 0.01\n",
      "Episode 379/500, epsilon: 0.01\n",
      "Episode 380/500, epsilon: 0.01\n",
      "Episode 381/500, epsilon: 0.01\n",
      "Episode 382/500, epsilon: 0.01\n",
      "Episode 383/500, epsilon: 0.01\n",
      "Episode 384/500, epsilon: 0.01\n",
      "Episode 385/500, epsilon: 0.01\n",
      "Episode 386/500, epsilon: 0.01\n",
      "Episode 387/500, epsilon: 0.01\n",
      "Episode 388/500, epsilon: 0.01\n",
      "Episode 389/500, epsilon: 0.01\n",
      "Episode 390/500, epsilon: 0.01\n",
      "Episode 391/500, epsilon: 0.01\n",
      "Episode 392/500, epsilon: 0.01\n",
      "Episode 393/500, epsilon: 0.01\n",
      "Episode 394/500, epsilon: 0.01\n",
      "Episode 395/500, epsilon: 0.01\n",
      "Episode 396/500, epsilon: 0.01\n",
      "Episode 397/500, epsilon: 0.01\n",
      "Episode 398/500, epsilon: 0.01\n",
      "Episode 399/500, epsilon: 0.01\n",
      "Episode 400/500, epsilon: 0.01\n",
      "Episode 401/500, epsilon: 0.01\n",
      "Episode 402/500, epsilon: 0.01\n",
      "Episode 403/500, epsilon: 0.01\n",
      "Episode 404/500, epsilon: 0.01\n",
      "Episode 405/500, epsilon: 0.01\n",
      "Episode 406/500, epsilon: 0.01\n",
      "Episode 407/500, epsilon: 0.01\n",
      "Episode 408/500, epsilon: 0.01\n",
      "Episode 409/500, epsilon: 0.01\n",
      "Episode 410/500, epsilon: 0.01\n",
      "Episode 411/500, epsilon: 0.01\n",
      "Episode 412/500, epsilon: 0.01\n",
      "Episode 413/500, epsilon: 0.01\n",
      "Episode 414/500, epsilon: 0.01\n",
      "Episode 415/500, epsilon: 0.01\n",
      "Episode 416/500, epsilon: 0.01\n",
      "Episode 417/500, epsilon: 0.01\n",
      "Episode 418/500, epsilon: 0.01\n",
      "Episode 419/500, epsilon: 0.01\n",
      "Episode 420/500, epsilon: 0.01\n",
      "Episode 421/500, epsilon: 0.01\n",
      "Episode 422/500, epsilon: 0.01\n",
      "Episode 423/500, epsilon: 0.01\n",
      "Episode 424/500, epsilon: 0.01\n",
      "Episode 425/500, epsilon: 0.01\n",
      "Episode 426/500, epsilon: 0.01\n",
      "Episode 427/500, epsilon: 0.01\n",
      "Episode 428/500, epsilon: 0.01\n",
      "Episode 429/500, epsilon: 0.01\n",
      "Episode 430/500, epsilon: 0.01\n",
      "Episode 431/500, epsilon: 0.01\n",
      "Episode 432/500, epsilon: 0.01\n",
      "Episode 433/500, epsilon: 0.01\n",
      "Episode 434/500, epsilon: 0.01\n",
      "Episode 435/500, epsilon: 0.01\n",
      "Episode 436/500, epsilon: 0.01\n",
      "Episode 437/500, epsilon: 0.01\n",
      "Episode 438/500, epsilon: 0.01\n",
      "Episode 439/500, epsilon: 0.01\n",
      "Episode 440/500, epsilon: 0.01\n",
      "Episode 441/500, epsilon: 0.01\n",
      "Episode 442/500, epsilon: 0.01\n",
      "Episode 443/500, epsilon: 0.01\n",
      "Episode 444/500, epsilon: 0.01\n",
      "Episode 445/500, epsilon: 0.01\n",
      "Episode 446/500, epsilon: 0.01\n",
      "Episode 447/500, epsilon: 0.01\n",
      "Episode 448/500, epsilon: 0.01\n",
      "Episode 449/500, epsilon: 0.01\n",
      "Episode 450/500, epsilon: 0.01\n",
      "Episode 451/500, epsilon: 0.01\n",
      "Episode 452/500, epsilon: 0.01\n",
      "Episode 453/500, epsilon: 0.01\n",
      "Episode 454/500, epsilon: 0.01\n",
      "Episode 455/500, epsilon: 0.01\n",
      "Episode 456/500, epsilon: 0.01\n",
      "Episode 457/500, epsilon: 0.01\n",
      "Episode 458/500, epsilon: 0.01\n",
      "Episode 459/500, epsilon: 0.01\n",
      "Episode 460/500, epsilon: 0.01\n",
      "Episode 461/500, epsilon: 0.01\n",
      "Episode 462/500, epsilon: 0.01\n",
      "Episode 463/500, epsilon: 0.01\n",
      "Episode 464/500, epsilon: 0.01\n",
      "Episode 465/500, epsilon: 0.01\n",
      "Episode 466/500, epsilon: 0.01\n",
      "Episode 467/500, epsilon: 0.01\n",
      "Episode 468/500, epsilon: 0.01\n",
      "Episode 469/500, epsilon: 0.01\n",
      "Episode 470/500, epsilon: 0.01\n",
      "Episode 471/500, epsilon: 0.01\n",
      "Episode 472/500, epsilon: 0.01\n",
      "Episode 473/500, epsilon: 0.01\n",
      "Episode 474/500, epsilon: 0.01\n",
      "Episode 475/500, epsilon: 0.01\n",
      "Episode 476/500, epsilon: 0.01\n",
      "Episode 477/500, epsilon: 0.01\n",
      "Episode 478/500, epsilon: 0.01\n",
      "Episode 479/500, epsilon: 0.01\n",
      "Episode 480/500, epsilon: 0.01\n",
      "Episode 481/500, epsilon: 0.01\n",
      "Episode 482/500, epsilon: 0.01\n",
      "Episode 483/500, epsilon: 0.01\n",
      "Episode 484/500, epsilon: 0.01\n",
      "Episode 485/500, epsilon: 0.01\n",
      "Episode 486/500, epsilon: 0.01\n",
      "Episode 487/500, epsilon: 0.01\n",
      "Episode 488/500, epsilon: 0.01\n",
      "Episode 489/500, epsilon: 0.01\n",
      "Episode 490/500, epsilon: 0.01\n",
      "Episode 491/500, epsilon: 0.01\n",
      "Episode 492/500, epsilon: 0.01\n",
      "Episode 493/500, epsilon: 0.01\n",
      "Episode 494/500, epsilon: 0.01\n",
      "Episode 495/500, epsilon: 0.01\n",
      "Episode 496/500, epsilon: 0.01\n",
      "Episode 497/500, epsilon: 0.01\n",
      "Episode 498/500, epsilon: 0.01\n",
      "Episode 499/500, epsilon: 0.01\n",
      "Episode 500/500, epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class Gomoku:\n",
    "    def __init__(self, size=5):  # Change the size to 5\n",
    "        self.size = size\n",
    "        self.board = np.zeros((size, size), dtype=int)\n",
    "        self.current_player = 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.board\n",
    "    \n",
    "    def is_valid_move(self, x, y):\n",
    "        return 0 <= x < self.size and 0 <= y < self.size and self.board[x, y] == 0\n",
    "    \n",
    "    def make_move(self, x, y):\n",
    "        if self.is_valid_move(x, y):\n",
    "            self.board[x, y] = self.current_player\n",
    "            self.current_player = 3 - self.current_player\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def check_winner(self):\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if self.board[x, y] != 0 and self.check_direction(x, y):\n",
    "                    return self.board[x, y]\n",
    "        return 0\n",
    "    \n",
    "    def check_direction(self, x, y):\n",
    "        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n",
    "        for d in directions:\n",
    "            count = 0\n",
    "            for i in range(-4, 5):\n",
    "                nx, ny = x + i * d[0], y + i * d[1]\n",
    "                if 0 <= nx < self.size and 0 <= ny < self.size and self.board[nx, ny] == self.board[x, y]:\n",
    "                    count += 1\n",
    "                    if count == 5:\n",
    "                        return True\n",
    "                else:\n",
    "                    count = 0\n",
    "        return False\n",
    "\n",
    "    def evaluate_position(self, player):\n",
    "        \"\"\"Evaluate the board from the perspective of the given player.\"\"\"\n",
    "        score = 0\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if self.board[x, y] == player:\n",
    "                    score += self.evaluate_point(x, y, player)\n",
    "                elif self.board[x, y] == 3 - player:\n",
    "                    score -= self.evaluate_point(x, y, 3 - player)\n",
    "        return score\n",
    "\n",
    "    def evaluate_point(self, x, y, player):\n",
    "        \"\"\"Evaluate a single point from the perspective of the given player.\"\"\"\n",
    "        score = 0\n",
    "        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n",
    "        for d in directions:\n",
    "            count = 0\n",
    "            block = 0\n",
    "            for i in range(-4, 5):\n",
    "                nx, ny = x + i * d[0], y + i * d[1]\n",
    "                if 0 <= nx < self.size and 0 <= ny < self.size:\n",
    "                    if self.board[nx, ny] == player:\n",
    "                        count += 1\n",
    "                    elif self.board[nx, ny] != 0:\n",
    "                        block += 1\n",
    "                        break\n",
    "                else:\n",
    "                    block += 1\n",
    "            if count == 5:\n",
    "                score += 10000  # win\n",
    "            elif count == 4 and block == 0:\n",
    "                score += 3000  # open four\n",
    "            elif count == 3 and block == 0:\n",
    "                score += 500  # open three\n",
    "            elif count == 2 and block == 0:\n",
    "                score += 1  # open two\n",
    "        return score\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size=128, gamma=0.99, lr=0.001, batch_size=64, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.model = DQN(state_size, hidden_size, action_size)\n",
    "        self.target_model = DQN(state_size, hidden_size, action_size)  # Add target network\n",
    "        self.update_target_model()  # Initialize target model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.get_valid_actions(state))\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        valid_actions = self.get_valid_actions(state)\n",
    "        return valid_actions[torch.argmax(q_values[0][valid_actions]).item()]\n",
    "    \n",
    "    def get_valid_actions(self, state):\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            state = state.numpy().flatten()  # Convert tensor to numpy array and flatten it\n",
    "        return [i for i in range(self.action_size) if state[i] == 0]\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * torch.max(self.target_model(next_state)).item()\n",
    "            target_f = self.model(state)\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(target_f, self.model(state))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "def train_dqn(agent, env, episodes=100):\n",
    "    for e in range(episodes):\n",
    "        state = env.reset().flatten()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            x, y = divmod(action, env.size)\n",
    "            if env.is_valid_move(x, y):\n",
    "                env.make_move(x, y)\n",
    "                reward = env.evaluate_position(1)\n",
    "                next_state = env.board.flatten()\n",
    "                done = env.check_winner() > 0 or (env.board != 0).all()\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "                # Opponent's turn (simple heuristic)\n",
    "                opp_action = random.choice(agent.get_valid_actions(torch.FloatTensor(env.board.flatten())))\n",
    "                opp_x, opp_y = divmod(opp_action, env.size)\n",
    "                env.make_move(opp_x, opp_y)\n",
    "                if env.check_winner():\n",
    "                    reward = -10000  # Large negative reward for losing\n",
    "                    done = True\n",
    "            else:\n",
    "                reward = -100  # Penalty for invalid move\n",
    "                done = True\n",
    "            agent.replay()\n",
    "        agent.update_target_model()  # Update target network periodically\n",
    "        print(f\"Episode {e+1}/{episodes}, epsilon: {agent.epsilon:.2f}\")\n",
    "    agent.save_model('dqn_gomoku.pth')\n",
    "\n",
    "gomoku_env = Gomoku(size=5)  # Set the size to 5x5\n",
    "dqn_agent = DQNAgent(state_size=gomoku_env.size*gomoku_env.size, action_size=gomoku_env.size*gomoku_env.size)\n",
    "train_dqn(dqn_agent, gomoku_env, episodes=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57237517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "class GomokuApp:\n",
    "    def __init__(self, root, size=5):\n",
    "        self.root = root\n",
    "        self.size = size\n",
    "        self.gomoku = Gomoku(size)\n",
    "        self.agent = DQNAgent(state_size=size*size, action_size=size*size)\n",
    "        self.agent.load_model('dqn_gomoku.pth')  # Uncomment if the model is already trained\n",
    "        self.canvas = tk.Canvas(root, width=500, height=500)\n",
    "        self.canvas.pack()\n",
    "        self.canvas.bind(\"<Button-1>\", self.on_click)\n",
    "        self.draw_board()\n",
    "        self.reset_game()\n",
    "    \n",
    "    def reset_game(self):\n",
    "        self.gomoku.reset()\n",
    "        self.update_canvas()\n",
    "    \n",
    "    def draw_board(self):\n",
    "        for i in range(self.size):\n",
    "            self.canvas.create_line(50 + i * 80, 50, 50 + i * 80, 450)\n",
    "            self.canvas.create_line(50, 50 + i * 80, 450, 50 + i * 80)\n",
    "    \n",
    "    def update_canvas(self):\n",
    "        self.canvas.delete(\"piece\")\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                if self.gomoku.board[x, y] == 1:\n",
    "                    self.canvas.create_oval(50 + x * 80 - 30, 50 + y * 80 - 30, 50 + x * 80 + 30, 50 + y * 80 + 30, fill=\"black\", tags=\"piece\")\n",
    "                elif self.gomoku.board[x, y] == 2:\n",
    "                    self.canvas.create_oval(50 + x * 80 - 30, 50 + y * 80 - 30, 50 + x * 80 + 30, 50 + y * 80 + 30, fill=\"red\", tags=\"piece\")\n",
    "    \n",
    "    def on_click(self, event):\n",
    "        x, y = (event.x - 50) // 80, (event.y - 50) // 80\n",
    "        if 0 <= x < self.size and 0 <= y < self.size and self.gomoku.is_valid_move(x, y):\n",
    "            self.gomoku.make_move(x, y)\n",
    "            self.update_canvas()\n",
    "            if self.gomoku.check_winner():\n",
    "                winner = self.gomoku.check_winner()\n",
    "                print(f\"Player {winner} wins!\")\n",
    "                self.reset_game()\n",
    "                return\n",
    "            self.agent_move()\n",
    "    \n",
    "    def agent_move(self):\n",
    "        state = self.gomoku.board.flatten()\n",
    "        action = self.agent.act(state)\n",
    "        x, y = divmod(action, self.size)\n",
    "        if self.gomoku.is_valid_move(x, y):\n",
    "            self.gomoku.make_move(x, y)\n",
    "            self.update_canvas()\n",
    "            if self.gomoku.check_winner():\n",
    "                winner = self.gomok\n",
    "                u.check_winner()\n",
    "                print(f\"Player {winner} wins!\")\n",
    "                self.reset_game()\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = GomokuApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f94b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = GomokuApp(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
